{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cdrann/random/blob/main/Pos_tagging_with_Bert_Fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt3MS34XRSWY"
      },
      "source": [
        "[BERT](https://arxiv.org/abs/1810.04805) is known to be good at Sequence tagging tasks like Named Entity Recognition. Let's see if it's true for POS-tagging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SVuwESoRSWi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch-pretrained-bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCbxtCmZRxZh",
        "outputId": "dac5b69d-3d1a-4ef7-95da-afb43f7e50cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-pretrained-bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from pytorch-pretrained-bert) (4.65.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.94-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.1/135.1 KB\u001b[0m \u001b[31m939.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from pytorch-pretrained-bert) (2022.10.31)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-pretrained-bert) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pytorch-pretrained-bert) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pytorch-pretrained-bert) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.5.0)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.94\n",
            "  Downloading botocore-1.29.94-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch-pretrained-bert) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch-pretrained-bert) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch-pretrained-bert) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch-pretrained-bert) (3.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.94->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.94->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.26.94 botocore-1.29.94 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaAn7ZrFRSWn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils import data\n",
        "from pytorch_pretrained_bert import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "SInBon-QRSWr",
        "outputId": "38c86905-02ca-4cb4-9898-865818dac1ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1+cu116'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZKbGwqIRSWv"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1F50HqpRSWw"
      },
      "source": [
        "Thanks to the great NLTK, we don't have to worry about datasets. Some of Penn Tree Banks are included in it. I believe they serves for the purpose."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('treebank')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVuattkuSDuo",
        "outputId": "9781863b-0701-40ca-b795-5d9eaf7bfcc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDP0D0iQRSWy",
        "outputId": "707bba1b-bf69-490f-fe99-3b93eebd557b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3914"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# The Penn Treebank Corpus:\n",
        "tagged_sents = nltk.corpus.treebank.tagged_sents()\n",
        "len(tagged_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulKrmUmwRSW0",
        "outputId": "e744ae62-fa7d-4e5e-956e-0915e258a623"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Pierre', 'NNP'),\n",
              " ('Vinken', 'NNP'),\n",
              " (',', ','),\n",
              " ('61', 'CD'),\n",
              " ('years', 'NNS'),\n",
              " ('old', 'JJ'),\n",
              " (',', ','),\n",
              " ('will', 'MD'),\n",
              " ('join', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('board', 'NN'),\n",
              " ('as', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('nonexecutive', 'JJ'),\n",
              " ('director', 'NN'),\n",
              " ('Nov.', 'NNP'),\n",
              " ('29', 'CD'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tagged_sents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXNGWOnARSW3"
      },
      "outputs": [],
      "source": [
        "tags = list(set(word_pos[1] for sent in tagged_sents for word_pos in sent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pcJsBQUvRSW6",
        "outputId": "16e6bee4-0277-4afe-a882-bf9a6d00b975"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"-NONE-,VB,NNS,PRP,PRP$,JJR,LS,JJ,#,VBZ,EX,$,VBN,RP,WRB,WP,FW,POS,VBG,DT,NNPS,NN,CC,MD,:,NNP,VBD,RBR,RBS,TO,JJS,WDT,WP$,VBP,RB,-LRB-,PDT,.,CD,,,'',IN,-RRB-,UH,``,SYM\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "\",\".join(tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af3ezsaORSW7"
      },
      "outputs": [],
      "source": [
        "# By convention, the 0'th slot is reserved for padding.\n",
        "tags = [\"<pad>\"] + tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X9NfFNIRSW-"
      },
      "outputs": [],
      "source": [
        "tag2idx = {tag:idx for idx, tag in enumerate(tags)}\n",
        "idx2tag = {idx:tag for idx, tag in enumerate(tags)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BAezJgvRSW_",
        "outputId": "061ef72d-a6a2-4a97-def8-d37944134e0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3522, 392)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Let's split the data into train and test (or eval)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_data, test_data = train_test_split(tagged_sents, test_size=.1)\n",
        "len(train_data), len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K10cJvunRSXG"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "L1-xoaoWFK8c",
        "outputId": "ca8e3ede-b715-495e-d10f-d1a74cdbaab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edEejJfeRSXH"
      },
      "source": [
        "# Data loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATh5mFWARSXI",
        "outputId": "010725a3-f0c0-48f5-c653-4caf7c71898b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 213450/213450 [00:00<00:00, 3297292.22B/s]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBAKQ5doRSXJ"
      },
      "outputs": [],
      "source": [
        "class PosDataset(data.Dataset):\n",
        "    def __init__(self, tagged_sents):\n",
        "        sents, tags_li = [], [] # list of lists\n",
        "        for sent in tagged_sents:\n",
        "            words = [word_pos[0] for word_pos in sent]\n",
        "            tags = [word_pos[1] for word_pos in sent]\n",
        "            sents.append([\"[CLS]\"] + words + [\"[SEP]\"])\n",
        "            tags_li.append([\"<pad>\"] + tags + [\"<pad>\"])\n",
        "        self.sents, self.tags_li = sents, tags_li\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n",
        "\n",
        "        # We give credits only to the first piece.\n",
        "        x, y = [], [] # list of ids\n",
        "        is_heads = [] # list. 1: the token is the first piece of a word\n",
        "        for w, t in zip(words, tags):\n",
        "            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n",
        "            xx = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "            is_head = [1] + [0]*(len(tokens) - 1)\n",
        "\n",
        "            t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n",
        "            yy = [tag2idx[each] for each in t]  # (T,)\n",
        "\n",
        "            x.extend(xx)\n",
        "            is_heads.extend(is_head)\n",
        "            y.extend(yy)\n",
        "\n",
        "        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n",
        "\n",
        "        # seqlen\n",
        "        seqlen = len(y)\n",
        "\n",
        "        # to string\n",
        "        words = \" \".join(words)\n",
        "        tags = \" \".join(tags)\n",
        "        return words, x, is_heads, tags, y, seqlen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_C7S3gnRSXL"
      },
      "outputs": [],
      "source": [
        "def pad(batch):\n",
        "    '''Pads to the longest sample'''\n",
        "    f = lambda x: [sample[x] for sample in batch]\n",
        "    words = f(0)\n",
        "    is_heads = f(2)\n",
        "    tags = f(3)\n",
        "    seqlens = f(-1)\n",
        "    maxlen = np.array(seqlens).max()\n",
        "\n",
        "    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n",
        "    x = f(1, maxlen)\n",
        "    y = f(-2, maxlen)\n",
        "\n",
        "\n",
        "    f = torch.LongTensor\n",
        "\n",
        "    return words, f(x), is_heads, tags, f(y), seqlens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a-WfhT9RSXP"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQn5rehHRSXX"
      },
      "outputs": [],
      "source": [
        "from pytorch_pretrained_bert import BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tf-C1y6hRSXX"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size=None):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "        self.fc = nn.Linear(768, vocab_size)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        '''\n",
        "        x: (N, T). int64\n",
        "        y: (N, T). int64\n",
        "        '''\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        \n",
        "        if self.training:\n",
        "            self.bert.train()\n",
        "            encoded_layers, _ = self.bert(x)\n",
        "            enc = encoded_layers[-1]\n",
        "        else:\n",
        "            self.bert.eval()\n",
        "            with torch.no_grad():\n",
        "                encoded_layers, _ = self.bert(x)\n",
        "                enc = encoded_layers[-1]\n",
        "        \n",
        "        logits = self.fc(enc)\n",
        "        y_hat = logits.argmax(-1)\n",
        "        return logits, y, y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00Y473yYRSXZ"
      },
      "source": [
        "# Train an evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aoTdsKQRSXc"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    model.train()\n",
        "    for i, batch in enumerate(iterator):\n",
        "        words, x, is_heads, tags, y, seqlens = batch\n",
        "        _y = y # for monitoring\n",
        "        optimizer.zero_grad()\n",
        "        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n",
        "\n",
        "        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n",
        "        y = y.view(-1)  # (N*T,)\n",
        "\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if i%10==0: # monitoring\n",
        "            print(\"step: {}, loss: {}\".format(i, loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NEwws70RSXe"
      },
      "outputs": [],
      "source": [
        "def eval(model, iterator):\n",
        "    model.eval()\n",
        "\n",
        "    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            words, x, is_heads, tags, y, seqlens = batch\n",
        "\n",
        "            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n",
        "\n",
        "            Words.extend(words)\n",
        "            Is_heads.extend(is_heads)\n",
        "            Tags.extend(tags)\n",
        "            Y.extend(y.numpy().tolist())\n",
        "            Y_hat.extend(y_hat.cpu().numpy().tolist())\n",
        "\n",
        "    ## gets results and save\n",
        "    with open(\"result\", 'w') as fout:\n",
        "        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n",
        "            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n",
        "            preds = [idx2tag[hat] for hat in y_hat]\n",
        "            assert len(preds)==len(words.split())==len(tags.split())\n",
        "            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n",
        "                fout.write(\"{} {} {}\\n\".format(w, t, p))\n",
        "            fout.write(\"\\n\")\n",
        "            \n",
        "    ## calc metric\n",
        "    y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n",
        "    y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n",
        "\n",
        "    acc = (y_true==y_pred).astype(np.int32).sum() / len(y_true)\n",
        "\n",
        "    print(\"acc=%.2f\"%acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UExaudsRSXg"
      },
      "source": [
        "## Load model and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymkzXUDZRSXh",
        "outputId": "5b1d7144-a05c-4697-84ee-11b3cb2ef351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 404400730/404400730 [00:10<00:00, 37145758.46B/s]\n"
          ]
        }
      ],
      "source": [
        "model = Net(vocab_size=len(tag2idx))\n",
        "model.to(device)\n",
        "model = nn.DataParallel(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKcRM_7DRSXh"
      },
      "outputs": [],
      "source": [
        "train_dataset = PosDataset(train_data)\n",
        "eval_dataset = PosDataset(test_data)\n",
        "\n",
        "train_iter = data.DataLoader(dataset=train_dataset,\n",
        "                             batch_size=8,\n",
        "                             shuffle=True,\n",
        "                             num_workers=1,\n",
        "                             collate_fn=pad)\n",
        "test_iter = data.DataLoader(dataset=eval_dataset,\n",
        "                             batch_size=8,\n",
        "                             shuffle=False,\n",
        "                             num_workers=1,\n",
        "                             collate_fn=pad)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X95qvV_MRSXl",
        "outputId": "7340e760-7ac2-4272-bc7e-16795db58d92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, loss: 3.8685035705566406\n",
            "step: 10, loss: 1.7828030586242676\n",
            "step: 20, loss: 0.7950708866119385\n",
            "step: 30, loss: 0.4256606996059418\n",
            "step: 40, loss: 0.23999132215976715\n",
            "step: 50, loss: 0.19955430924892426\n",
            "step: 60, loss: 0.3931425213813782\n",
            "step: 70, loss: 0.08938563615083694\n",
            "step: 80, loss: 0.18984177708625793\n",
            "step: 90, loss: 0.13247619569301605\n",
            "step: 100, loss: 0.19631338119506836\n",
            "step: 110, loss: 0.09652449190616608\n",
            "step: 120, loss: 0.1010337769985199\n",
            "step: 130, loss: 0.08772258460521698\n",
            "step: 140, loss: 0.1630076766014099\n",
            "step: 150, loss: 0.14914600551128387\n",
            "step: 160, loss: 0.044792525470256805\n",
            "step: 170, loss: 0.06629658490419388\n",
            "step: 180, loss: 0.09267449378967285\n",
            "step: 190, loss: 0.12260664999485016\n",
            "step: 200, loss: 0.14829736948013306\n",
            "step: 210, loss: 0.11935346573591232\n",
            "step: 220, loss: 0.1254226416349411\n",
            "step: 230, loss: 0.12320947647094727\n",
            "step: 240, loss: 0.10381089150905609\n",
            "step: 250, loss: 0.06362539529800415\n",
            "step: 260, loss: 0.1427251547574997\n",
            "step: 270, loss: 0.13380655646324158\n",
            "step: 280, loss: 0.09337376058101654\n",
            "step: 290, loss: 0.10933277010917664\n",
            "step: 300, loss: 0.1421898752450943\n",
            "step: 310, loss: 0.10232862830162048\n",
            "step: 320, loss: 0.11981827020645142\n",
            "step: 330, loss: 0.17513491213321686\n",
            "step: 340, loss: 0.16852721571922302\n",
            "step: 350, loss: 0.07903903722763062\n",
            "step: 360, loss: 0.11889434605836868\n",
            "step: 370, loss: 0.07945016026496887\n",
            "step: 380, loss: 0.061896927654743195\n",
            "step: 390, loss: 0.04448628053069115\n",
            "step: 400, loss: 0.0640878900885582\n",
            "step: 410, loss: 0.1282646209001541\n",
            "step: 420, loss: 0.15653300285339355\n",
            "step: 430, loss: 0.2047446072101593\n",
            "step: 440, loss: 0.050340987741947174\n",
            "acc=0.97\n"
          ]
        }
      ],
      "source": [
        "train(model, train_iter, optimizer, criterion)\n",
        "eval(model, test_iter)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval(model, test_iter)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxyLvvNYg7b2",
        "outputId": "134f9cec-16ab-48cb-f1ac-52ce3370249e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc=0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCJcie8uRSXm"
      },
      "source": [
        "Check the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6-OOkwjRSXn",
        "outputId": "011e343c-fb7b-4865-d5d6-9379e5a6b204"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Meanwhile RB RB',\n",
              " ', , ,',\n",
              " 'the DT DT',\n",
              " 'National NNP NNP',\n",
              " 'Association NNP NNP',\n",
              " 'of IN IN',\n",
              " 'Purchasing NNP NNP',\n",
              " 'Management NNP NNP',\n",
              " 'said VBD VBD',\n",
              " '0 -NONE- -NONE-',\n",
              " 'its PRP$ PRP$',\n",
              " 'latest JJS JJS',\n",
              " 'survey NN NN',\n",
              " 'indicated VBD VBD',\n",
              " 'that IN IN',\n",
              " 'the DT DT',\n",
              " 'manufacturing NN NN',\n",
              " 'economy NN NN',\n",
              " 'contracted VBD VBN',\n",
              " 'in IN IN',\n",
              " 'October NNP NNP',\n",
              " 'for IN IN',\n",
              " 'the DT DT',\n",
              " 'sixth JJ JJ',\n",
              " 'consecutive JJ JJ',\n",
              " 'month NN NN',\n",
              " '. . .',\n",
              " '',\n",
              " 'Lancaster NNP NNP',\n",
              " 'Colony NNP NNP',\n",
              " 'Corp. NNP NNP',\n",
              " 'said VBD VBD',\n",
              " '0 -NONE- -NONE-',\n",
              " 'it PRP PRP',\n",
              " 'acquired VBD VBD',\n",
              " 'Reames NNP NNP',\n",
              " 'Foods NNP NNPS',\n",
              " 'Inc. NNP NNP',\n",
              " 'in IN IN',\n",
              " 'a DT DT',\n",
              " 'cash NN NN',\n",
              " 'transaction NN NN',\n",
              " '. . .',\n",
              " '',\n",
              " 'Weatherly NNP NNP',\n",
              " 'Securities NNP NNPS',\n",
              " 'Corp. NNP NNP',\n",
              " ', , ,',\n",
              " 'New NNP NNP',\n",
              " 'York NNP NNP',\n",
              " ', , ,',\n",
              " 'and CC CC',\n",
              " 'three CD CD',\n",
              " 'of IN IN',\n",
              " 'its PRP$ PRP$',\n",
              " 'principals NNS NNS',\n",
              " '-- : :',\n",
              " 'Dell NNP NNP',\n",
              " 'Eugene NNP NNP',\n",
              " 'Keehn NNP NNP',\n",
              " 'and CC CC',\n",
              " 'William NNP NNP',\n",
              " 'Northy NNP NNP',\n",
              " 'Prater NNP NNP',\n",
              " 'Jr. NNP NNP',\n",
              " ', , ,',\n",
              " 'both DT DT',\n",
              " 'of IN IN',\n",
              " 'Mercer NNP NNP',\n",
              " 'Island NNP NNP',\n",
              " ', , ,',\n",
              " 'Wash. NNP NNP',\n",
              " ', , ,',\n",
              " 'and CC CC',\n",
              " 'Thomas NNP NNP',\n",
              " 'Albert NNP NNP',\n",
              " 'McFall NNP NNP',\n",
              " ', , ,',\n",
              " 'of IN IN',\n",
              " 'Red NNP NNP',\n",
              " 'Bank NNP NNP',\n",
              " ', , ,',\n",
              " 'N.J NNP NNP',\n",
              " '. . .',\n",
              " '-- : :',\n",
              " 'consented VBD VBD',\n",
              " 'to TO TO',\n",
              " 'a DT DT',\n",
              " 'fine NN NN',\n",
              " 'of IN IN',\n",
              " '$ $ $',\n",
              " '20,000 CD CD',\n",
              " '*U* -NONE- -NONE-',\n",
              " '. . .',\n",
              " '',\n",
              " 'Trinity NNP NNP',\n",
              " 'Industries NNPS NNPS',\n",
              " 'Inc. NNP NNP',\n",
              " 'said VBD VBD',\n",
              " '0 -NONE- -NONE-']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "open('result', 'r').read().splitlines()[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyADSbhVRSXo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}